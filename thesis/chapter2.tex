\فصل{مفاهیم پایه}

در این فصل قصد داریم مفاهیم اولیه موردنیاز در این رساله را به اختصار مرور کنیم.
در ابتدا بسیار کوتاه به مفاهیم پایه‌ای چون گراف‌ها و شبکه‌های عصبی می‌پردازیم و در ادامه وارد
بحث شبکه‌های گرافی و ارتباطات مکانی می‌شویم.

\قسمت{شبکه‌های عصبی مصنوعی}
یک شبکه عصبی مصنوعی مبتنی بر مجموعه‌ای از واحدها یا گره‌های متصل بهم به نام نورون‌های مصنوعی است که نورون‌ها را در یک مغز بیولوژیکی مدل‌سازی می‌کند.
هر اتصال، مانند سیناپس های یک مغز بیولوژیکی، می تواند سیگنالی را به نورون های دیگر منتقل کند.
یک نورون مصنوعی سیگنالی را دریافت می‌کند و سپس آن را پردازش می‌کند و می‌تواند به نورون‌های متصل به آن سیگنال دهد.
``سیگنال'' در یک اتصال یک عدد حقیقی است و خروجی هر نورون با توسط تابعی غیرخطی که بر روی مجموع ورودی‌های آن اعمال می‌گردد، محاسبه می‌شود. اتصالات لبه نامیده می‌شود.
نورون‌ها و لبه‌ها معمولاً وزنی دارند که در حین یادگیری تنظیم می‌شوند. وزن باعث افزایش یا کاهش قدرت سیگنال در یک اتصال می‌شود. نورون‌ها ممکن است آستانه‌ای داشته باشند، که سیگنال تنها در صورتی ارسال شود که سیگنال کل از آن آستانه عبور کند.
به طور معمول، نورون‌ها در لایه‌ها جمع می‌شوند. لایه‌های مختلف ممکن است تبدیل‌های متفاوتی را روی ورودی‌های خود انجام دهند.
سیگنال ها از اولین لایه (لایه ورودی)، احتمالا پس از چندین بار عبور از لایه‌ها به آخرین لایه (لایه خروجی) منتقل می‌شوند.

\قسمت{پیچش}
اصطلاح پیچش به ترکیب ریاضی دو تابع برای تولید تابع سوم اشاره دارد که این دو مجموعه از اطلاعات را ادغام می کند.

\begin{equation}
  \label{eq:conv}
(f \text{*} g)(t) \triangleq \int_{-\infty}^\infty f(\tau)g(t - \tau)d\tau
\end{equation}


\begin{table}[h]
  \centering
  \caption{توضیح پارامترهای رابطه (\رجوع{eq:conv})}
  \begin{tabular}{|c|p{0.5\textwidth}|}
    \hline
    $(f \text{*} g)(t)$ & توابعی که در حال پیچش هستند \\
    \hline
    $t$ & متغیر عددی حقیقی توابع \متن‌لاتین{f}  و \متن‌لاتین{g} \\
    \hline
  \end{tabular}
  \label{tbl:distance}
\end{table}

\شروع{شکل}
  \درج‌تصویر[width=\textwidth]{./images/convolution_base.png}
  \تنظیم‌ازوسط
  \شرح{پیچش}
  \برچسب{fig:signal-conv}
\پایان{شکل}

\قسمت{شبکه‌ی عصبی پیچشی}
شبکه‌های عصبی پیچشی نوعی تخصصی از شبکه‌های عصبی مصنوعی هستند که حداقل در یکی از لایه‌های خود از عملیات پیچش به جای ضرب ماتریس استفاده می‌کنند.
آنها به طور خاص برای پردازش داده‌های پیکسل طراحی شده‌اند و در تشخیص و پردازش تصویر استفاده می‌شوند.
یک شبکه عصبی پیچشی از یک لایه ورودی، لایه های پنهان و یک لایه خروجی تشکیل شده است. در یک شبکه عصبی پیچشی، لایه‌های پنهان شامل لایه‌هایی هستند که پیچش را انجام می‌دهند. معمولاً این لایه‌ها حاصل ضرب نقطه‌ای هسته‌ی پیچش را با ماتریس ورودی لایه انجام می دهند. این ضرب معمولا ضرب داخلی فروبنیوس است. همانطور که هسته‌ی پیچش در امتداد ماتریس ورودی لایه می لغزد، عملیات پیچش یک نقشه ویژگی ایجاد می کند که به نوبه خود به ورودی لایه بعدی کمک می کند. به دنبال آن لایه‌های دیگری مانند لایه‌های \متن‌لاتین{pooling}، لایه‌های کاملاً متصل و لایه‌های نرمال‌سازی قرار می‌گیرند.

\شروع{شکل}
  \درج‌تصویر[width=\textwidth]{./images/convolution.png}
  \تنظیم‌ازوسط
  \شرح{پیچش با استفاده از کرنل}
  \برچسب{fig:time-conv}
\پایان{شکل}

\قسمت{گراف}
گراف ساختاری است شامل مجموعه‌ای از اشیاء که در آن برخی از جفت اشیاء به نوعی ``مرتبط'' هستند. اشیاء با انتزاعات ریاضی به نام رئوس مطابقت دارند (که گره ها یا نقاط نیز نامیده می شوند) و هر یک از جفت رئوس مرتبط یک یال نامیده می‌شود (همچنین پیوند یا خط نیز نامیده می شود).

\قسمت{شبکه‌ی عصبی گرافی در دامنه‌ی مکانی}

این حالت متشکل از دو تابع است. تابع اول تجمیع کننده است که اطلاعات را از گره‌های همسایه‌می‌گیرد و آن‌ها را خلاصه می‌کند. تابع دوم به روزرسانی است که اطلاعات گره مورد بحث را با خروجی تجمیع‌کننده ترکیب می‌کند و به بازنمایی جدیدی از گره می‌رسد.

\begin{equation}
  \label{eq:spatial}
H_{:v}^{(l+1)}=upd(f(H_{:v}^{(l)}),agg(g(H_{:u}^{(l)}): u\in \mathcal{N}(v)))
\end{equation}

\begin{table}[h]
  \centering
  \caption{توضیح پارامترهای رابطه (\رجوع{eq:spatial})}
  \begin{tabular}{|c|p{0.5\textwidth}|}
    \hline
    $H_{:v}^{(l)}$ & بازنمایی گره $u$ در مرحله‌ی $l$ \\
    \hline
    $\mathcal{N}(v)$ & گره‌های همسایه‌ی گره $v$ \\
    \hline
    $agg$ & تابع تجمیع‌کننده \\
    \hline
    $upd$ & تابع به روزرسانی \\
    \hline
  \end{tabular}
  \label{tbl:distance}
\end{table}

در اکثر موارد می‌توانیم شبکه‌ی عصبی گرافی مکانی را به وسیله‌ی چارچوب کلی زیر پیاده‌سازی کنیم:

\begin{equation}
  \label{eq:spatial_1}
H^{(l+1)}=\sigma(\sum_{s}C^{(s)}H^{(l)}W^{(l,s)})
\end{equation}

\begin{table}[h]
  \centering
  \caption{توضیح پارامترهای رابطه (\رجوع{eq:spatial_1})}
  \begin{tabular}{|c|p{0.5\textwidth}|}
    \hline
    $C$ & \متن‌لاتین{convolution support} \\
    \hline
    $H$ & ویژگی‌های گره \\
    \hline
    $W$ & پارامترهای قابل آموزش \\
    \hline
  \end{tabular}
  \label{tbl:distance}
\end{table}

\قسمت{شبکه‌ی عصبی گرافی در دامنه‌ی طیفی}

در این حالت تابع به صورت زیر تعریف می‌شود:

\begin{equation}
  \label{eq:spectral}
H_j^{l+1}=\sigma(\sum_{i=1}^{f_l}U\:diag(F_{i,j,l})U^TH_i^{(l)})
\end{equation}

\begin{table}[h]
  \centering
  \caption{توضیح پارامترهای رابطه (\رجوع{eq:spectral})}
  \begin{tabular}{|c|p{0.5\textwidth}|}
    \hline
    $U$ & ماتریس بردار ویژه‌ها \\
    \hline
    $F$ & فیلتر قابل یادگیری \\
    \hline
    $H$ & ویژگی‌های گره \\
    \hline
  \end{tabular}
  \label{tbl:distance}
\end{table}

ار آن جایی که تجزیه‌ی ویژه از نظر محاسباتی بسیار سنگین است، عمومی‌ترین راه این است که وزن قابل یادگیری را پارامتری کنیم:

$F_{i,j,l}=B[W_{i,j}^{(l,1)},...,W_{i,j}^{(l,S)}]^T$

$B$ نشان می‌دهد که پارامتری کردن چگونه باید انجام شود. می‌توان نشان داد که شبکه‌ی عصبی گرافی طیفی که با استفاده از ماتریس $B$ پارامتری شده است در حقیقت یک مورد خاص از شبکه‌ی عصبی گرافی مکانی است که هسته‌ی پیچش به صورت زیر قرار داده شده است:

$C^{(s)}=U\:diag(F_s(\lambda))U^T$

\قسمت{واحد بازگشتی دروازه‌ای}

واحدهای بازگشتی دروازه‌ای یک مکانیزم دروازه‌ای در شبکه‌های عصبی بازرخدادی هستند.
واحد بازگشتی دروازه‌ای مانند یک حافظه‌ی کوتاه مدت ماندگار با دروازه‌ی فراموشی است اما پارامترهای کم‌تری نسبت به آن دارد، زیرا فاقد گیت خروجی است.
عملکرد واحد بازگشتی دروازه‌ای در برخی از وظایف مانند مدل‌سازی موسیقی چندصدایی، مدل‌سازی سیگنال گفتار و پردازش زبان طبیعی مشابه عملکرد حافظه‌ی کوتاه مدت ماندگار است.
دو نسخه‌ی ساده و کامل از حافظه‌ی بازگشتی دروازه‌ای وجود دارد که روابط نسخه‌ی ساده‌ی آن در ادامه آورده شده است:

\begin{equation}
  \label{eq:gru}
  \begin{aligned}
  f_{t}& = \sigma _{g} ( W_{f} x_{t} + U_{f} h_{t-1} + b_{f} )\\
  {\hat {h}}_{t}& = \phi _{h}(W_{h}x_{t}+U_{h}(f_{t}\odot h_{t-1})+b_{h})\\
  h_{t}& = (1-f_{t})\odot h_{t-1}+f_{t}\odot {\hat {h}}_{t}
  \end{aligned}
\end{equation}

\begin{table}[h]
  \centering
  \caption{توضیح پارامترهای رابطه (\رجوع{eq:gru})}
  \begin{tabular}{|c|p{0.5\textwidth}|}
    \hline
    $x_{t}$ & بردار ورودی \\
    \hline
    $h_{t}$ & بردار خروجی \\
    \hline
    ${\hat {h}}_{t}$ & بردار فعالسازی کاندید شده \\
    \hline
    $f_{t}$ & بردار فراموشی \\
    \hline
    $W, U ,b$ & ماتریس و بردار پارامترها \\
    \hline
  \end{tabular}
  \label{tbl:distance}
\end{table}

\قسمت{جمع‌بندی}
در طول گزارش از برخی مفاهیم پایه استفاده خواهیم کرد. در این فصل برای درک سریع‌تر، این مفاهیم پایه مانند پیچش، تفاوت تحلیل در دامنه‌ی مکانی با دامنه‌ی طیفی و واحد بازگشتی دروازه‌ای را مورد بررسی قرار دادیم.

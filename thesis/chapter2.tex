\فصل{مفاهیم پایه}

\قسمت{پیچش}
اصطلاح پیچش به ترکیب ریاضی دو تابع برای تولید تابع سوم اشاره دارد که این دو مجموعه از اطلاعات را ادغام می کند.

\begin{equation}
  \label{eq:conv}
(f \text{*} g)(t) \triangleq \int_{-\infty}^\infty f(\tau)g(t - \tau)\tau'
\end{equation}


\begin{table}[h]
  \centering
  \caption{توضیح پارامترهای رابطه \رجوع{eq:conv}}
  \begin{tabular}{|c|p{0.5\textwidth}|}
    \hline
    $(f \text{*} g)(t)$ & توابعی که در حال پیچش هستند \\
    \hline
    $t$ & متغیر عددی حقیقی توابع \متن‌لاتین{f}  و \متن‌لاتین{g} \\
    \hline
    $g(\tau)$ & پیچش تابع $f(t)$ \\
    \hline
    $\tau'$ & مشتق اول تابع $g(\tau)$ \\
    \hline
  \end{tabular}
  \label{tbl:distance}
\end{table}

در مورد شبکه‌ی عصبی پیچشی، پیچش روی داده‌های ورودی با استفاده از یک فیلتر یا هسته انجام می‌شود (این اصطلاحات به جای یکدیگر استفاده می‌شوند) تا سپس یک نقشه ویژگی تولید شود. وقتی یک پیچش را با کشیدن فیلتر روی ورودی اجرا می کنیم. در هر مکان، یک ضرب ماتریس انجام می شود و نتیجه را بر روی نقشه ویژگی جمع می کند.

\شروع{شکل}
  \درج‌تصویر[height=8cm]{./images/convolution.png}
  \تنظیم‌ازوسط
  \شرح{پیچش}
  \برچسب{fig:time-conv}
\پایان{شکل}

\قسمت{شبکه‌ی عصبی گرافی در دامنه‌ی مکانی}

این حالت متشکل از دو تابع است. تابع اول تجمیع کننده است که اطلاعات را از گره‌های همسایه‌می‌گیرد و آن‌ها را خلاصه می‌کند. تابع دوم به روزرسانی است که اطلاعات گره مورد بحث را با خروجی تجمیع‌کننده ترکیب می‌کند و به بازنمایی جدیدی از گره می‌رسد.


\begin{equation}
  \label{eq:spatial}
H_{:v}^{(l+1)}=upd(f(H_{:v}^{(l)}),agg(g(H_{:u}^{(l)}): u\in \mathcal{N}(v)))
\end{equation}


\begin{table}[h]
  \centering
  \caption{توضیح پارامترهای رابطه \رجوع{eq:spatial}}
  \begin{tabular}{|c|p{0.5\textwidth}|}
    \hline
    $H_{:v}^{(l)}$ & بازنمایی گره $u$ در مرحله‌ی $l$ \\
    \hline
    $\mathcal{N}(v)$ & گره‌های همسایه‌ی گره $v$ \\
    \hline
    $agg$ & تابع تجمیع‌کننده \\
    \hline
    $upd$ & تابع به روزرسانی \\
    \hline
  \end{tabular}
  \label{tbl:distance}
\end{table}


در اکثر موارد می‌توانیم شبکه‌ی عصبی گرافی مکانی را به وسیله‌ی چارچوب کلی زیر پیاده‌سازی کنیم


\begin{equation}
  \label{eq:spatial}
H^{(l+1)}=\sigma(\sum_{s}C^{(s)}H^{(l)}W^{(l,s)})
\end{equation}


\begin{table}[h]
  \centering
  \caption{توضیح پارامترهای رابطه \رجوع{eq:spatial}}
  \begin{tabular}{|c|p{0.5\textwidth}|}
    \hline
    $C$ & \متن‌لاتین{convolution support} \\
    \hline
    $H$ & ویژگی‌های گره \\
    \hline
    $W$ & پارامترهای قابل آموزش \\
    \hline
  \end{tabular}
  \label{tbl:distance}
\end{table}


\قسمت{شبکه‌ی عصبی گرافی در دامنه‌ی طیفی}

در این حالت تابع به صورت زیر تعریف می‌شود:

\begin{equation}
  \label{eq:spectral}
H_j^{l+1}=\sigma(\sum_{i=1}^{f_l}U\:diag(F_{i,j,l})U^TH_i^{(l)})
\end{equation}


\begin{table}[h]
  \centering
  \caption{توضیح پارامترهای رابطه \رجوع{eq:spatial}}
  \begin{tabular}{|c|p{0.5\textwidth}|}
    \hline
    $U$ & ماتریس بردار ویژه‌ها \\
    \hline
    $F$ & فیلتر قابل یادگیری \\
    \hline
    $H$ & ویژگی‌های گره \\
    \hline
  \end{tabular}
  \label{tbl:distance}
\end{table}

ار آن جایی که تجزیه‌ی ویژه از نظر محاسباتی بسیار سنگین است، عمومی‌ترین راه این است که وزن قابل یادگیری را پارامتری کنیم

$F_{i,j,l}=B[W_{i,j}^{(l,1)},...,W_{i,j}^{(l,S)}]^T$

$B$ نشان می‌دهد که پارامتری کردن چگونه باید انجام شود. می‌توان نشان داد که شبکه‌ی عصبی گرافی طیفی که با استفاده از ماتریس $B$ پارامتری شده است در حقیقت یک مورد خاص از شبکه‌ی عصبی گرافی مکانی است که هسته‌ی پیچش به صورت زیر قرار داده شده است

$C^{(s)}=U\:diag(F_s(\lambda))U^T$

\قسمت{واحد بازگشتی دروازه‌ای}
واحدهای بازگشتی دروازه‌ای یک مکانیزم دروازه‌ای در شبکه‌های عصبی بازرخدادی هستند. واحد بازگشتی دروازه‌ای مانند یک حافظه‌ی کوتاه مدت ماندگار با دروازه‌ی فراموشی است اما پارامترهای کم‌تری نسبت به آن دارد، زیرا فاقد گیت خروجی است. عملکرد واحد بازگشتی دروازه‌ای در برخی از وظایف مانند مدل‌سازی موسیقی چندصدایی، مدل‌سازی سیگنال گفتار و پردازش زبان طبیعی مشابه عملکرد حافظه‌ی کوتاه مدت ماندگار است. دو نسخه‌ی ساده و کامل از حافظه‌ی بازگشتی دروازه‌ای وجود دارد که روابط نسخه‌ی ساده‌ی آن در ادامه آورده شده است:


\begin{equation}
  \label{eq:base}
  \begin{aligned}
  f_{t}& = \sigma _{g} ( W_{f} x_{t} + U_{f} h_{t-1} + b_{f} )\\
  {\hat {h}}_{t}& = \phi _{h}(W_{h}x_{t}+U_{h}(f_{t}\odot h_{t-1})+b_{h})\\
  h_{t}& = (1-f_{t})\odot h_{t-1}+f_{t}\odot {\hat {h}}_{t}
  \end{aligned}
\end{equation}


\begin{table}[h]
  \centering
  \caption{توضیح پارامترهای رابطه \رجوع{eq:time-conv}}
  \begin{tabular}{|c|p{0.5\textwidth}|}
    \hline
    $x_{t}$ & بردار ورودی \\
    \hline
    $h_{t}$ & بردار خروجی \\
    \hline
    ${\hat {h}}_{t}$ & بردار فعالسازی کاندید شده \\
    \hline
    $f_{t}$ & بردار فراموشی \\
    \hline
    $W, U ,b$ & ماتریس و بردار پارامترها \\
    \hline
  \end{tabular}
  \label{tbl:distance}
\end{table}

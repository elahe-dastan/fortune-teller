{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code and all my understandings and everything about magic. This is somehow my first serious work in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# TODO: make these constants arguments\n",
    "n_route = 228\n",
    "n_hist = 12\n",
    "n_pred = 9\n",
    "batch_size = 50\n",
    "epoch = 50\n",
    "# parser.add_argument('--save', type=int, default=10)\n",
    "ks = 3  # spatial convolution kernel size\n",
    "kt = 3  # temporal convolution kernel size\n",
    "lr = 1e-3\n",
    "opt = torch.optim.RMSprop\n",
    "# parser.add_argument('--graph', type=str, default='default')\n",
    "inf_mode = 'merge'\n",
    "rate = 0.1\n",
    "save = 10\n",
    "Ko = 4\n",
    "# Ko changes with n_hist and Kt -> Ko = n_hist - 4 * Kt\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has two spatio temporal blocks and each block has two gated temporal convolutional layers and one spatial convolutional layer in between so we have totaly six layers and we should determine the size of channel for these layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = [[1, 32, 64], [64, 32, 128]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should have a file which defines the graph we want to work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228, 228])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = pd.read_csv(\"PeMSD7_W_228.csv\", header=None)\n",
    "W_tensor = torch.tensor(W.values)\n",
    "W_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation 10 on original papaer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_matrix(W, sigma2=0.1, epsilon=0.5, scaling=True):\n",
    "    '''\n",
    "    Weight matrix function.\n",
    "    :param W: tensor, the adjacency matrix of the graph.\n",
    "    :param sigma2: float, scalar of matrix W.\n",
    "    :param epsilon: float, thresholds to control the sparsity of matrix W.\n",
    "    :param scaling: bool, whether applies numerical scaling on W.\n",
    "    :return: np.ndarray, [n_route, n_route].\n",
    "    '''\n",
    "    # check whether W is a 0/1 matrix.\n",
    "    if set(torch.unique(W)) == {0, 1}:\n",
    "        print('The input graph is a 0/1 matrix; set \"scaling\" to False.')\n",
    "        scaling = False\n",
    "\n",
    "    if scaling:\n",
    "        n = W.shape[0]\n",
    "        W = W / 10000. # WHY\n",
    "        W2, W_mask = W * W, np.ones([n, n]) - np.identity(n)\n",
    "        # refer to Eq.10\n",
    "        a = torch.exp(-W2 / sigma2)\n",
    "        a[a >= epsilon] = 0\n",
    "        \n",
    "        return a * W_mask\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000e+00, 1.4279e-89, 1.0224e-88,  ..., 4.9829e-01, 4.9897e-01,\n",
      "        4.9999e-01], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "tmp = weight_matrix(W_tensor)\n",
    "print(torch.unique(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below cell I want to calculate normalized laplacian.<br/>\n",
    "I calculate it using \\(L=I - D^(-1/2) W D^(-1/2)\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some how different from the researchers code needs more investigation\n",
    "\n",
    "def scaled_laplacian(W):\n",
    "    '''\n",
    "    Scaled graph Laplacian function.\n",
    "    :param W: np.ndarray, [n_route, n_route], weighted adjacency matrix of G.\n",
    "    :return: np.matrix, [n_route, n_route].\n",
    "    '''\n",
    "    # d ->  diagonal degree matrix\n",
    "    n, d = np.shape(W)[0], torch.sum(W, axis=1)\n",
    "    I = torch.eye(n)\n",
    "    \n",
    "    # d^(-1/2)\n",
    "    d = torch.pow(d, -1/2)\n",
    "    # make d diagonal\n",
    "    d = torch.diag(d)\n",
    "\n",
    "    dw = torch.matmul(d, W)\n",
    "    dwd = torch.matmul(dw, d)\n",
    "    \n",
    "    L = I - dwd\n",
    "    \n",
    "    # lambda_max \\approx 2.0, the largest eigenvalues of L.\n",
    "    lambdas, _ = torch.eig(L)\n",
    "    lambdas = lambdas[:, 0]\n",
    "    lambda_max = torch.max(lambdas)\n",
    "\n",
    "    return 2 * L / lambda_max - I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = scaled_laplacian(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7208, -0.1258, -0.1659, -0.1780, -0.2509, -0.2819],\n",
      "        [-0.1258,  0.7208, -0.2224, -0.2863, -0.3027, -0.3239],\n",
      "        [-0.1659, -0.2224,  0.7208, -0.3460, -0.3548, -0.3702],\n",
      "        [-0.1780, -0.2863, -0.3460,  0.3665, -0.3567, -0.3665],\n",
      "        [-0.2509, -0.3027, -0.3548, -0.3567,  0.3409, -0.3874],\n",
      "        [-0.2819, -0.3239, -0.3702, -0.3665, -0.3874,  0.3269]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "T = torch.tensor([[0, 2, 3, 4, 6, 7], [2, 0, 5, 8, 9, 10], [3, 5, 0, 11, 12, 13], \n",
    "                  [4, 8, 11, 14, 15, 16], [6, 9, 12, 15, 17, 18], [7, 10, 13, 16, 18, 19]], dtype=float)\n",
    "T = scaled_laplacian(T)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chebyshev polynomials approximation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually this is half of cheb poly\n",
    "def cheb_poly_approx(L, Ks, n):\n",
    "    '''\n",
    "    Chebyshev polynomials approximation function.\n",
    "    :param L: np.matrix, [n_route, n_route], graph Laplacian.\n",
    "    :param Ks: int, kernel size of spatial convolution.\n",
    "    :param n: int, number of routes / size of graph.\n",
    "    :return: np.ndarray, [n_route, Ks*n_route].\n",
    "    '''\n",
    "    L0, L1 = torch.eye(n), L.detach().clone()\n",
    "\n",
    "    if Ks > 1:\n",
    "        L_list = [L0.detach().clone(), L1.detach().clone()]\n",
    "        for i in range(Ks - 2):\n",
    "            Ln = 2 * L * L1 - L0\n",
    "            L_list.append(Ln.detach().clone())\n",
    "            L0, L1 = L1.detach().clone(), Ln.detach().clone()\n",
    "        # L_lsit [Ks, n*n], Lk [n, Ks*n]\n",
    "        return torch.cat(L_list, axis=-1)\n",
    "    elif Ks == 1:\n",
    "        return L0\n",
    "    else:\n",
    "        raise ValueError(f'ERROR: the size of spatial kernel must be greater than 1, but received \"{Ks}\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approximation method: 1st approx - first_approx(W, n).\n",
    "Lk = cheb_poly_approx(L, ks, n_route)\n",
    "Lk = Lk.to(device)\n",
    "Lk = Lk.type(torch.cuda.FloatTensor)\n",
    "# Lk = Lk.type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_file = pd.read_csv(\"PeMSD7_V_228.csv\", header=None)\n",
    "data_file_tensor = torch.tensor(data_file.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_val, n_test = 34, 5, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_gen(len_seq, data_seq, offset, n_frame, n_route, day_slot, C_0=1):\n",
    "    '''\n",
    "    Generate data in the form of standard sequence unit.\n",
    "    :param len_seq: int, the length of target date sequence.  is the number of days\n",
    "    :param data_seq: np.ndarray, source data / time-series.\n",
    "    :param offset:  int, the starting index of different dataset type.\n",
    "    :param n_frame: int, the number of frame within a standard sequence unit,\n",
    "                         which contains n_his = 12 and n_pred = 9 (3 /15 min, 6 /30 min & 9 /45 min).\n",
    "    :param n_route: int, the number of routes in the graph.\n",
    "    :param day_slot: int, the number of time slots per day, controlled by the time window (5 min as default).\n",
    "    :param C_0: int, the size of input channel.\n",
    "    :return: np.ndarray, [len_seq, , C_0, n_frame, n_route].\n",
    "    '''\n",
    "    # in this example time step is 5 min so a day which is 24 hours will be 288 time steps (day_slot=288) \n",
    "    # and n_frame is 21 so we will have 288 - 21 + 1 = 268 (n_slot=268)\n",
    "    n_slot = day_slot - n_frame + 1\n",
    "\n",
    "    tmp_seq = torch.zeros((len_seq * n_slot, C_0, n_frame, n_route))\n",
    "    for i in range(len_seq):\n",
    "        for j in range(n_slot):\n",
    "            sta = (i + offset) * day_slot + j\n",
    "            end = sta + n_frame\n",
    "            tmp_seq[i * n_slot + j, :, :, :] = torch.reshape(data_seq[sta:end, :], [C_0, n_frame, n_route])\n",
    "    return tmp_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(x, mean, std):\n",
    "    '''\n",
    "    Z-score normalization function: $z = (X - \\mu) / \\sigma $,\n",
    "    where z is the z-score, X is the value of the element,\n",
    "    $\\mu$ is the population mean, and $\\sigma$ is the standard deviation.\n",
    "    :param x: np.ndarray, input array to be normalized.\n",
    "    :param mean: float, the value of mean.\n",
    "    :param std: float, the value of standard deviation.\n",
    "    :return: np.ndarray, z-score normalized array.\n",
    "    '''\n",
    "    return (x - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, data, stats):\n",
    "        self.__data = data\n",
    "        self.mean = stats['mean']\n",
    "        self.std = stats['std']\n",
    "\n",
    "    def get_data(self, type):\n",
    "        return self.__data[type]\n",
    "\n",
    "    def get_stats(self):\n",
    "        return {'mean': self.mean, 'std': self.std}\n",
    "\n",
    "    def get_len(self, type):\n",
    "        return len(self.__data[type])\n",
    "\n",
    "    def z_inverse(self, type):\n",
    "        return self.__data[type] * self.std + self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(data_seq, data_config, n_route, n_frame=21, day_slot=288):\n",
    "    '''\n",
    "    Source file load and dataset generation.\n",
    "    :param file_path: str, the file path of data source.\n",
    "    :param data_config: tuple, the configs of dataset in train, validation, test.\n",
    "    :param n_route: int, the number of routes in the graph.\n",
    "    :param n_frame: int, the number of frame within a standard sequence unit,\n",
    "                         which contains n_his = 12 and n_pred = 9 (3 /15 min, 6 /30 min & 9 /45 min).\n",
    "    :param day_slot: int, the number of time slots per day, controlled by the time window (5 min as default).\n",
    "    :return: dict, dataset that contains training, validation and test with stats.\n",
    "    '''\n",
    "    n_train, n_val, n_test = data_config\n",
    "    # generate training, validation and test data\n",
    "#     data_seq = data.values\n",
    "    \n",
    "    seq_train = seq_gen(n_train, data_seq, 0, n_frame, n_route, day_slot)\n",
    "    seq_val = seq_gen(n_val, data_seq, n_train, n_frame, n_route, day_slot)\n",
    "    seq_test = seq_gen(n_test, data_seq, n_train + n_val, n_frame, n_route, day_slot)\n",
    "\n",
    "    # x_stats: dict, the stats for the train dataset, including the value of mean and standard deviation.\n",
    "    x_stats = {'mean': torch.mean(seq_train), 'std': torch.std(seq_train)}\n",
    "    print(x_stats)\n",
    "\n",
    "    # x_train, x_val, x_test: np.array, [sample_size, n_frame, n_route, channel_size].\n",
    "    x_train = z_score(seq_train, x_stats['mean'], x_stats['std'])\n",
    "    x_val = z_score(seq_val, x_stats['mean'], x_stats['std'])\n",
    "    x_test = z_score(seq_test, x_stats['mean'], x_stats['std'])\n",
    "    \n",
    "    x_train = x_train.to(device)\n",
    "    \n",
    "    print(\"++++++++\")\n",
    "    print(x_train.type())\n",
    "\n",
    "    x_data = {'train': x_train, 'val': x_val, 'test': x_test}\n",
    "    dataset = Dataset(x_data, x_stats)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.DoubleTensor\n",
      "{'mean': tensor(58.4998), 'std': tensor(13.7286)}\n",
      "++++++++\n",
      "torch.cuda.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "print(data_file_tensor.type())\n",
    "PeMS = data_gen(data_file_tensor, (n_train, n_val, n_test), n_route, n_hist + n_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem\n",
    "\n",
    "def layer_norm(x):\n",
    "    '''\n",
    "    Layer normalization function.\n",
    "    :param x: tensor, [batch_size, channel, time_step, n_route].\n",
    "    :return: tensor, [batch_size, channel, time_step, n_route].\n",
    "    '''\n",
    "    _, C, _, N = x.size()\n",
    "    mu, sigma = torch.mean(x, dim=[2, 3], keepdim=True), torch.var(x, dim=[2, 3], keepdim=True)\n",
    "\n",
    "    gamma = torch.ones([1, C, 1, N])\n",
    "    beta = torch.zeros([1, C, 1, N])\n",
    "    # big problem\n",
    "    _x = (x - mu) / torch.sqrt(sigma + 1e-6) # * gamma + beta\n",
    "    \n",
    "    return _x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 2, 3])\n",
      "tensor([[[[1.4851, 1.6399, 1.7947],\n",
      "          [1.9495, 2.1043, 2.2591]]],\n",
      "\n",
      "\n",
      "        [[[3.3428, 3.4976, 3.6524],\n",
      "          [3.8072, 3.9620, 4.1168]]]], grad_fn=<ThnnConv2DBackward>)\n",
      "torch.Size([2, 1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "test = torch.Tensor([[[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]], [[[13, 14, 15], [16, 17, 18]], \n",
    "                                                                           [[19, 20, 21], [22, 23, 24]]]])\n",
    "\n",
    "print(test.size())\n",
    "a = torch.nn.Conv2d(2, 1, (1, 1), stride=[1, 1])(test)\n",
    "print(a)\n",
    "print(a.size())\n",
    "# print(layer_norm(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ST_Network(torch.nn.Module):\n",
    "    def __init__(self, n_his, Ks, Kt, rate, Lk, Ko):\n",
    "        '''\n",
    "        Build the base model.\n",
    "        :param n_his: int, size of historical records for training.\n",
    "        :param Ks: int, kernel size of spatial convolution.\n",
    "        :param Kt: int, kernel size of temporal convolution.\n",
    "        :param blocks: list, channel configs of st_conv blocks.\n",
    "        :param rate: the rate of dropout.\n",
    "        '''\n",
    "        super(ST_Network, self).__init__()\n",
    "        self.n_his = n_his\n",
    "        self.Ks = Ks\n",
    "        self.Kt = Kt\n",
    "        self.rate = rate\n",
    "        self.Lk = Lk\n",
    "        self.Ko = Ko\n",
    "        \n",
    "        print(\"self.Lk\")\n",
    "        print(self.Lk.type())\n",
    "        \n",
    "        # blocks = [1, 32, 64], [64, 32, 128]\n",
    "        # take a look at the link below for quick reminder of padding, stride and dilation\n",
    "        # https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md \n",
    "        # stride=[1, 1] means no stride\n",
    "        # padding=[0, 0] means no padding or \"Valid\"\n",
    "        # dilation=[1, 1] means no dilation\n",
    "        self.Conv2DBlock1Temporal1GLU = torch.nn.Conv2d(1, 2 * 32, (Kt, 1), stride=[1, 1], padding=[0, 0], dilation=[1, 1])\n",
    "        self.SigmoidBlock1Temporal1 = torch.nn.Sigmoid()\n",
    "        \n",
    "        self.ThetaBlock1Spatio = torch.nn.Parameter(torch.rand((self.Ks * 32, 32), requires_grad=True)) # problem device and dtype\n",
    "        self.ReLUBlock1Spatio = torch.nn.ReLU()\n",
    "        \n",
    "        self.Conv2DBlock1Temporal2ReLU = torch.nn.Conv2d(32, 64, (Kt, 1), stride=[1, 1], padding=[0, 0], dilation=[1, 1])\n",
    "        self.ReLUBlock1Temporal2 = torch.nn.ReLU()\n",
    "        \n",
    "        self.DropoutBlock1 = torch.nn.Dropout(p=rate)\n",
    "        \n",
    "        # problem\n",
    "        # the size of kernel is one so SAME padding is meaningless\n",
    "        self.Conv2DBlock2Temporal1 = torch.nn.Conv2d(64, 32, (1, 1), stride=[1, 1])\n",
    "        self.Conv2DBlock2Temporal1GLU = torch.nn.Conv2d(64, 2 * 32, (Kt, 1), stride=[1, 1], padding=[0, 0], dilation=[1, 1])\n",
    "        self.SigmoidBlock2Temporal1 = torch.nn.Sigmoid()\n",
    "        \n",
    "        self.ThetaBlock2Spatio = torch.nn.Parameter(torch.rand((self.Ks * 32, 32), requires_grad=True)) # problem device and dtype\n",
    "        self.ReLUBlock2Spatio = torch.nn.ReLU()\n",
    "        \n",
    "        self.Conv2DBlock2Temporal2ReLU = torch.nn.Conv2d(32, 128, (Kt, 1), stride=[1, 1], padding=[0, 0], dilation=[1, 1])\n",
    "        self.ReLUBlock2Temporal2 = torch.nn.ReLU()\n",
    "        \n",
    "        self.DropoutBlock2 = torch.nn.Dropout(p=rate)\n",
    "        \n",
    "        self.Conv2DOutputTemporal1GLU = torch.nn.Conv2d(128, 2 * 128, (Ko, 1), stride=[1, 1], padding=[0, 0], dilation=[1, 1])\n",
    "        self.SigmoidOutputTemporal1 = torch.nn.Sigmoid()\n",
    "        self.Conv2DOutputTemporal2Sigmoid = torch.nn.Conv2d(128, 128, (1, 1), stride=[1, 1], padding=[0, 0], dilation=[1, 1])\n",
    "        self.SigmoidOutputTemporal2 = torch.nn.Sigmoid()\n",
    "        # problem\n",
    "        # the size of kernel is one so SAME padding is meaningless\n",
    "        self.FullyConnectedLayer = torch.nn.Conv2d(128, 1, (1, 1), stride=[1, 1], dilation=[1, 1])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Ko>0: kernel size of temporal convolution in the output layer.\n",
    "        Ko = self.n_his\n",
    "        \n",
    "        print(\"forward\")\n",
    "        print(x.type())\n",
    "        \n",
    "        # ST-Block\n",
    "        # [1, 32, 64]\n",
    "        x = self.st_conv_block1(x)\n",
    "        \n",
    "        # [64, 32, 128]\n",
    "        x = self.st_conv_block2(x)         \n",
    "\n",
    "        # Output Layer\n",
    "        y = self.output_layer(x)\n",
    "\n",
    "        #     tf.add_to_collection(name='copy_loss',\n",
    "        #                          value=tf.nn.l2_loss(y - inputs[:, n_his:n_his + 1, :, :]))\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "#         train_loss = loss_fn(y, inputs[:, :, n_his:n_his + 1, :])\n",
    "#         single_pred = y[:, 0, :, :]\n",
    "        #     tf.add_to_collection(name='y_pred', value=single_pred)\n",
    "#         return train_loss, single_pred\n",
    "        return y\n",
    "\n",
    "    def st_conv_block1(self, x):\n",
    "        '''\n",
    "        Spatio-temporal convolutional block, which contains two temporal gated convolution layers\n",
    "        and one spatial graph convolution layer in the middle.\n",
    "        :param x: tensor, batch_size, time_step, n_route, c_in].\n",
    "        :param Ks: int, kernel size of spatial convolution.\n",
    "        :param Kt: int, kernel size of temporal convolution.\n",
    "        :param channels: list, channel configs of a single st_conv block.\n",
    "        :param scope: str, variable scope.\n",
    "        :param keep_prob: placeholder, prob of dropout.\n",
    "        :param act_func: str, activation function.\n",
    "        :return: tensor, [batch_size, time_step, n_route, c_out].\n",
    "        '''\n",
    "        \n",
    "        x_s = self.block1Temporal1(x)\n",
    "        x_t = self.block1Spatio(x_s)\n",
    "        x_o = self.block1Temporal2(x_t)\n",
    "        x_ln = layer_norm(x_o)\n",
    "    \n",
    "        return self.DropoutBlock1(x_ln)\n",
    "    \n",
    "    def block1Temporal1(self, x):\n",
    "        '''\n",
    "        Temporal convolution layer.\n",
    "        :param x: tensor, [batch_size, c_in, time_step, n_route].\n",
    "        :param Kt: int, kernel size of temporal convolution.\n",
    "        :param c_in: int, size of input channel.\n",
    "        :param c_out: int, size of output channel.\n",
    "        :param act_func: str, activation function.\n",
    "        :return: tensor, [batch_size, c_out, time_step-Kt+1, n_route].\n",
    "        '''\n",
    "\n",
    "        _, _, T, n = x.size()\n",
    "\n",
    "        # if the size of input channel is less than the output,\n",
    "        # padding x to the same size of output channel.\n",
    "        # Note, _.get_shape() cannot convert a partially known TensorShape to a Tensor.\n",
    "        x_input = torch.cat([x, torch.zeros([x.size()[0], 32 - 1, T, n]).to(device)], axis=1)\n",
    "        \n",
    "        # keep the original input for residual connection.\n",
    "        # this is a bad code. We want to use this for residual connections and in order to do \n",
    "        # that the dimentions should match so we use \"self.Kt - 1:T\" just to fix this problem \n",
    "        # but we are actually just deleting the first time steps which actually makes a little \n",
    "        # sense that we are careing about nearer time steps \n",
    "        x_input = x_input[:, :, self.Kt - 1:T, :]\n",
    "        \n",
    "        # gated liner unit\n",
    "    #   tf.add_to_collection(name='weight_decay', value=tf.nn.l2_loss(wt))\n",
    "    #   is it really the same as padding= 'valid'\n",
    "        x_conv = self.Conv2DBlock1Temporal1GLU(x)\n",
    "        \n",
    "        p = (x_conv[:, 0:32, :, :] + x_input)\n",
    "        q = x_conv[:, -32:, :, :]\n",
    "\n",
    "        return p * self.SigmoidBlock1Temporal1(q)\n",
    "    \n",
    "    def block1Spatio(self, x):\n",
    "        '''\n",
    "        Spatial graph convolution layer.\n",
    "        :param x: tensor, [batch_size, c_in, time_step, n_route].\n",
    "        :param Ks: int, kernel size of spatial convolution.\n",
    "        :param c_in: int, size of input channel.\n",
    "        :param c_out: int, size of output channel.\n",
    "        :return: tensor, [batch_size, c_out, time_step, n_route].\n",
    "        '''\n",
    "        _, _, T, n = x.size()\n",
    "        \n",
    "        x_input = x\n",
    "\n",
    "    #     variable_summaries(ws, 'theta')\n",
    "        bs = torch.zeros((32))\n",
    "        # x -> [batch_size, c_in, time_step, n_route] -> [batch_size, time_step, c_in, n_route]\n",
    "        # x -> [batch_size, time_step, c_in, n_route] -> [batch_size*time_step, c_in, n_route]\n",
    "        # x -> [batch_size*time_step, c_in, n_route] -> [batch_size*time_step, c_out, n_route]\n",
    "        a = torch.reshape(torch.transpose(x, 1, 2), [-1, 32, n])\n",
    "        x_gconv = self.gconv(a, self.ThetaBlock1Spatio, self.Ks, 32, 32, self.Lk)# + bs\n",
    "        # [batch_size*time_step, c_out, n_route] -> [batch_size, time_step, c_out, n_route]\n",
    "        # [batch_size, time_step, c_out, n_route] -> [batch_size, c_out, time_step, n_route]\n",
    "        x_gc = torch.transpose(torch.reshape(x_gconv, [-1, T, 32, n]), 1, 2)\n",
    "        \n",
    "        return self.ReLUBlock1Spatio(x_gc[:, :, :, :] + x_input)\n",
    "    \n",
    "    def block1Temporal2(self, x):\n",
    "        '''\n",
    "        Temporal convolution layer.\n",
    "        :param x: tensor, [batch_size, c_in, time_step, n_route].\n",
    "        :param Kt: int, kernel size of temporal convolution.\n",
    "        :param c_in: int, size of input channel.\n",
    "        :param c_out: int, size of output channel.\n",
    "        :param act_func: str, activation function.\n",
    "        :return: tensor, [batch_size, c_out, time_step-Kt+1, n_route].\n",
    "        '''\n",
    "\n",
    "        _, _, T, n = x.size()\n",
    "\n",
    "        # if the size of input channel is less than the output,\n",
    "        # padding x to the same size of output channel.\n",
    "        # Note, _.get_shape() cannot convert a partially known TensorShape to a Tensor.\n",
    "        x_input = torch.cat([x, torch.zeros([x.size()[0], 64 - 32, T, n]).to(device)], axis=1)\n",
    "        \n",
    "        # keep the original input for residual connection.\n",
    "        x_input = x_input[:, :, self.Kt - 1:T, :]\n",
    "        \n",
    "        x_conv = self.Conv2DBlock1Temporal2ReLU(x)\n",
    "        \n",
    "        return self.ReLUBlock1Temporal2(x_conv + x_input)\n",
    "\n",
    "    def st_conv_block2(self, x):\n",
    "        '''\n",
    "        Spatio-temporal convolutional block, which contains two temporal gated convolution layers\n",
    "        and one spatial graph convolution layer in the middle.\n",
    "        :param x: tensor, batch_size, time_step, n_route, c_in].\n",
    "        :param Ks: int, kernel size of spatial convolution.\n",
    "        :param Kt: int, kernel size of temporal convolution.\n",
    "        :param channels: list, channel configs of a single st_conv block.\n",
    "        :param scope: str, variable scope.\n",
    "        :param keep_prob: placeholder, prob of dropout.\n",
    "        :param act_func: str, activation function.\n",
    "        :return: tensor, [batch_size, time_step, n_route, c_out].\n",
    "        '''     \n",
    "        x_s = self.block2Temporal1(x)\n",
    "        x_t = self.block2Spatio(x_s)\n",
    "        x_o = self.block2Temporal2(x_t)\n",
    "        x_ln = layer_norm(x_o)\n",
    "    \n",
    "        return self.DropoutBlock2(x_ln)\n",
    "    \n",
    "    def block2Temporal1(self, x):\n",
    "        '''\n",
    "        Temporal convolution layer.\n",
    "        :param x: tensor, [batch_size, c_in, time_step, n_route].\n",
    "        :param Kt: int, kernel size of temporal convolution.\n",
    "        :param c_in: int, size of input channel.\n",
    "        :param c_out: int, size of output channel.\n",
    "        :param act_func: str, activation function.\n",
    "        :return: tensor, [batch_size, c_out, time_step-Kt+1, n_route].\n",
    "        '''\n",
    "        _, _, T, n = x.size()\n",
    "\n",
    "#       PyTorch does not support same padding the way Keras does\n",
    "#       padding='SAME'\n",
    "        x_input = self.Conv2DBlock2Temporal1(x)\n",
    "\n",
    "        \n",
    "        # keep the original input for residual connection.\n",
    "        x_input = x_input[:, :, self.Kt - 1:T, :]\n",
    "        \n",
    "        # gated liner unit\n",
    "    #   tf.add_to_collection(name='weight_decay', value=tf.nn.l2_loss(wt))\n",
    "    #   is it really the same as padding= 'valid'\n",
    "        x_conv = self.Conv2DBlock2Temporal1GLU(x)\n",
    "        \n",
    "        p = (x_conv[:, 0:32, :, :] + x_input)\n",
    "        q = x_conv[:, -32:, :, :]\n",
    "\n",
    "        return p * self.SigmoidBlock2Temporal1(q)  \n",
    "\n",
    "    def block2Spatio(self, x):\n",
    "        '''\n",
    "        Spatial graph convolution layer.\n",
    "        :param x: tensor, [batch_size, c_in, time_step, n_route].\n",
    "        :param Ks: int, kernel size of spatial convolution.\n",
    "        :param c_in: int, size of input channel.\n",
    "        :param c_out: int, size of output channel.\n",
    "        :return: tensor, [batch_size, c_out, time_step, n_route].\n",
    "        '''\n",
    "        _, _, T, n = x.size()\n",
    "        \n",
    "        x_input = x\n",
    "\n",
    "    #     variable_summaries(ws, 'theta')\n",
    "        bs = torch.zeros((32))\n",
    "        # x -> [batch_size, c_in, time_step, n_route] -> [batch_size, time_step, c_in, n_route]\n",
    "        # x -> [batch_size, time_step, c_in, n_route] -> [batch_size*time_step, c_in, n_route]\n",
    "        # x -> [batch_size*time_step, c_in, n_route] -> [batch_size*time_step, c_out, n_route]\n",
    "        x_gconv = self.gconv(torch.reshape(torch.transpose(x, 1, 2), [-1, 32, n]), self.ThetaBlock2Spatio, \n",
    "                             self.Ks, 32, 32, self.Lk)# + bs problem\n",
    "        # [batch_size*time_step, c_out, n_route] -> [batch_size, time_step, c_out, n_route]\n",
    "        # [batch_size, time_step, c_out, n_route] -> [batch_size, c_out, time_step, n_route]\n",
    "        x_gc = torch.transpose(torch.reshape(x_gconv, [-1, T, 32, n]), 1, 2)\n",
    "        \n",
    "        return self.ReLUBlock2Spatio(x_gc[:, :, :, :] + x_input)\n",
    "        \n",
    "    def block2Temporal2(self, x):\n",
    "        '''\n",
    "        Temporal convolution layer.\n",
    "        :param x: tensor, [batch_size, c_in, time_step, n_route].\n",
    "        :param Kt: int, kernel size of temporal convolution.\n",
    "        :param c_in: int, size of input channel.\n",
    "        :param c_out: int, size of output channel.\n",
    "        :param act_func: str, activation function.\n",
    "        :return: tensor, [batch_size, c_out, time_step-Kt+1, n_route].\n",
    "        '''   \n",
    "        _, _, T, n = x.size()\n",
    "\n",
    "        # if the size of input channel is less than the output,\n",
    "        # padding x to the same size of output channel.\n",
    "        # Note, _.get_shape() cannot convert a partially known TensorShape to a Tensor.\n",
    "        x_input = torch.cat([x, torch.zeros([x.size()[0], 128 - 32, T, n]).to(device)], axis=1)\n",
    "        \n",
    "        # keep the original input for residual connection.\n",
    "        x_input = x_input[:, :, self.Kt - 1:T, :]\n",
    "        \n",
    "        x_conv = self.Conv2DBlock2Temporal2ReLU(x)\n",
    "\n",
    "        return self.ReLUBlock2Temporal2(x_conv + x_input)\n",
    "\n",
    "    def output_layer(self, x):\n",
    "        '''\n",
    "        Output layer: temporal convolution layers attach with one fully connected layer,\n",
    "        which map outputs of the last st_conv block to a single-step prediction.\n",
    "        :param x: tensor, [batch_size, channel, time_step, n_route].\n",
    "        :param T: int, kernel size of temporal convolution.\n",
    "        :param scope: str, variable scope.\n",
    "        :param act_func: str, activation function.\n",
    "        :return: tensor, [batch_size, 1, n_route, 1].\n",
    "        '''\n",
    "        # maps multi-steps to one.\n",
    "        x_i = self.output_layer_temp1_GLU(x)\n",
    "        x_ln = layer_norm(x_i)\n",
    "        x_o = self.output_layer_temp2_sigmoid(x_ln)            \n",
    "            \n",
    "        # maps multi-channels to one.\n",
    "        x_fc = self.fully_con_layer(x_o)\n",
    "        \n",
    "        return x_fc\n",
    "    \n",
    "    def output_layer_temp1_GLU(self, x):\n",
    "        '''\n",
    "        Temporal convolution layer.\n",
    "        :param x: tensor, [batch_size, c_in, time_step, n_route].\n",
    "        :param Kt: int, kernel size of temporal convolution.\n",
    "        :param c_in: int, size of input channel.\n",
    "        :param c_out: int, size of output channel.\n",
    "        :param act_func: str, activation function.\n",
    "        :return: tensor, [batch_size, c_out, time_step-Kt+1, n_route].\n",
    "        '''\n",
    "        _, _, T, n = x.size()\n",
    "        \n",
    "        x_input = x\n",
    "        \n",
    "        # keep the original input for residual connection.\n",
    "        x_input = x_input[:, :, self.Ko - 1:T, :]\n",
    "        \n",
    "        # gated liner unit\n",
    "    #   tf.add_to_collection(name='weight_decay', value=tf.nn.l2_loss(wt))\n",
    "    #   is it really the same as padding= 'valid'\n",
    "        x_conv = self.Conv2DOutputTemporal1GLU(x)\n",
    "        \n",
    "        p = (x_conv[:, 0:128, :, :] + x_input)\n",
    "        q = x_conv[:, -128:, :, :]\n",
    "\n",
    "        return p * self.SigmoidOutputTemporal1(q)\n",
    "    \n",
    "    def output_layer_temp2_sigmoid(self, x):\n",
    "        '''\n",
    "        Temporal convolution layer.\n",
    "        :param x: tensor, [batch_size, c_in, time_step, n_route].\n",
    "        :param Kt: int, kernel size of temporal convolution.\n",
    "        :param c_in: int, size of input channel.\n",
    "        :param c_out: int, size of output channel.\n",
    "        :param act_func: str, activation function.\n",
    "        :return: tensor, [batch_size, c_out, time_step-Kt+1, n_route].\n",
    "        '''\n",
    "        _, _, T, n = x.size()\n",
    "        \n",
    "        x_conv = self.Conv2DOutputTemporal2Sigmoid(x)\n",
    "        \n",
    "        return self.SigmoidOutputTemporal2(x_conv)\n",
    "\n",
    "    def fully_con_layer(self, x):\n",
    "        '''\n",
    "        Fully connected layer: maps multi-channels to one.\n",
    "        :param x: tensor, [batch_size, channel, 1, n_route].\n",
    "        :param n: int, number of route / size of graph.\n",
    "        :param channel: channel size of input x.\n",
    "        :return: tensor, [batch_size, 1, n_route, 1].\n",
    "        '''\n",
    "    #     tf.add_to_collection(name='weight_decay', value=tf.nn.l2_loss(w))\n",
    "    #     PyTorch does not support same padding the way Keras does\n",
    "    #     padding='SAME'\n",
    "        return self.FullyConnectedLayer(x)\n",
    "    \n",
    "    # problem\n",
    "    def gconv(self, x, theta, Ks, c_in, c_out, kernel):\n",
    "        '''\n",
    "        Spectral-based graph convolution function.\n",
    "        :param x: tensor, [batch_size, c_in, n_route].\n",
    "        :param theta: tensor, [Ks*c_in, c_out], trainable kernel parameters.\n",
    "        :param Ks: int, kernel size of graph convolution.\n",
    "        :param c_in: int, size of input channel.\n",
    "        :param c_out: int, size of output channel.\n",
    "        :return: tensor, [batch_size, n_route, c_out].\n",
    "        '''\n",
    "        # graph kernel: tensor, [n_route, Ks*n_route]\n",
    "        n = kernel.shape[0]\n",
    "        # x -> [batch_size, c_in, n_route] -> [batch_size*c_in, n_route]\n",
    "        x_tmp = torch.reshape(x, [-1, n])\n",
    "        # x_mul = x_tmp * ker -> [batch_size*c_in, Ks*n_route] -> [batch_size, c_in, Ks, n_route]\n",
    "        print(\"x_tmp\")\n",
    "        print(x_tmp.type())\n",
    "        print(\"kernel\")\n",
    "        print(kernel.type())\n",
    "        x_mul = torch.reshape(torch.matmul(x_tmp, kernel), [-1, c_in, Ks, n]) # problem should kernel's \n",
    "        # requires_grad be true and be on GPU\n",
    "        # [batch_size, c_in, Ks, n_route] -> [batch_size, n_route, Ks, c_in]\n",
    "        # [batch_size, n_route, Ks, c_in] -> [batch_size, n_route, c_in, Ks]\n",
    "        # x_ker -> [batch_size, n_route, c_in, K_s] -> [batch_size*n_route, c_in*Ks]\n",
    "        x_ker = torch.reshape(torch.transpose(torch.transpose(x_mul, 1, 3), 2, 3), [-1, c_in * Ks])\n",
    "        # x_gconv -> [batch_size*n_route, c_out] -> [batch_size, n_route, c_out]\n",
    "        print(\"x_ker\")\n",
    "        print(x_ker.type())\n",
    "        print(\"theta\")\n",
    "        print(theta.type())\n",
    "        x_gconv = torch.reshape(torch.matmul(x_ker, theta), [-1, n, c_out])\n",
    "        # [batch_size, n_route, c_out] -> [batch_size, c_out, n_route]\n",
    "        x_gconv = torch.transpose(x_gconv, 1, 2) # I added this line, maybe we can also reshape to [-1, c_out, n] in \n",
    "        # the previous line instead\n",
    "        return x_gconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.Lk\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "model = ST_Network(n_hist, ks, kt, rate, Lk, Ko).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_step(model, loss_fn, optimizer):\n",
    "    def train_step(x, y):\n",
    "        # put model in training mode \n",
    "        # some models may use mechanisms like Dropout, for instance, which have distinct behaviors in \n",
    "        # training and evaluation phase\n",
    "        model.train()\n",
    "        \n",
    "        # compute the model output\n",
    "        yhat = model(x)\n",
    "        \n",
    "        # calculate loss\n",
    "        print(type(yhat))\n",
    "        print(yhat.size())\n",
    "#         print(\"x_batch[:, :, n_hist-2:, :]\")\n",
    "#         print(x_batch[:, :, n_hist-2:, :].size())\n",
    "        # todo the dimention of yhat is wrong. its dimention 2 value should be 9 not 10\n",
    "        loss = loss_fn(y, yhat)\n",
    "        print(loss)\n",
    "        \n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "    \n",
    "        # update model weights\n",
    "        optimizer.step()  \n",
    "        # clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    # Returns the function that will be calculated inside the train loop\n",
    "    return train_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inputs, model, epoch, n_hist):\n",
    "    # Define model loss\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    # define the optimization\n",
    "    # torch.optim.Adam(lr, weight_decay=0.01).minimize(train_loss)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    \n",
    "    train_step = make_train_step(model, criterion, optimizer)\n",
    "    losses = []\n",
    "    \n",
    "    # enumerate epochs     \n",
    "    for i in range(epoch):\n",
    "        start_time = time.time()\n",
    "        data_loader = DataLoader(inputs.get_data('train'),\n",
    "                                 batch_size=batch_size, # where did I define batch size\n",
    "                                 drop_last=True)\n",
    "        # gen_batch(inputs.get_data('train'), batch_size, dynamic_batch=True, shuffle=True)\n",
    "        # enumerate mini batches\n",
    "        for _, x_batch in enumerate(data_loader):\n",
    "            # x dimention = [batch_size, channel = 1, n_frame, n_route]\n",
    "            # n_hist = 12\n",
    "            x = x_batch[:, :, 0:n_hist, :]\n",
    "            \n",
    "            # Performs one train step and returns the corresponding loss\n",
    "            loss = train_step(x, x_batch[:, :, n_hist, :])\n",
    "            losses.append(loss)\n",
    "            break # remove\n",
    "        \n",
    "#         print(model.state_dict())\n",
    "        print(f'Epoch {i:2d} Training Time {time.time() - start_time:.3f}s')\n",
    "\n",
    "        start_time = time.time()\n",
    "#         min_va_val, min_val = \\\n",
    "#                 model_inference(sess, pred, inputs, batch_size, n_his, n_pred, step_idx, min_va_val, min_val)\n",
    "\n",
    "            \n",
    "#         loss = loss_fn(predictions, t)\n",
    "#       train_op.step()\n",
    "#         for ix in tmp_idx:\n",
    "#             va, te = min_va_val[ix - 2:ix + 1], min_val[ix - 2:ix + 1]\n",
    "#             print(f'Time Step {ix + 1}: '\n",
    "#                     f'MAPE {va[0]:7.3%}, {te[0]:7.3%}; '\n",
    "#                     f'MAE  {va[1]:4.3f}, {te[1]:4.3f}; '\n",
    "#                     f'RMSE {va[2]:6.3f}, {te[2]:6.3f}.')\n",
    "#         print(f'Epoch {i:2d} Inference Time {time.time() - start_time:.3f}s')\n",
    "\n",
    "#         if (i + 1) % save == 0 || (i + 1) == epoch:\n",
    "        torch.save(model.state_dict(), \"model.pt\")\n",
    "        break # remove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n",
      "torch.FloatTensor\n",
      "x_tmp\n",
      "torch.FloatTensor\n",
      "kernel\n",
      "torch.FloatTensor\n",
      "x_ker\n",
      "torch.FloatTensor\n",
      "theta\n",
      "torch.FloatTensor\n",
      "x_tmp\n",
      "torch.FloatTensor\n",
      "kernel\n",
      "torch.FloatTensor\n",
      "x_ker\n",
      "torch.FloatTensor\n",
      "theta\n",
      "torch.FloatTensor\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([50, 1, 1, 228])\n",
      "tensor(0.6563, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raha/anaconda3/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([50, 1, 1, 228])) that is different to the input size (torch.Size([50, 1, 228])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 Training Time 0.759s\n"
     ]
    }
   ],
   "source": [
    "train(PeMS, model, epoch, n_hist)\n",
    "# todo the problem was model.parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# import sys\n",
    "\n",
    "# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tensorflow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def multi_pred(model, y_pred, seq, batch_size, n_his, n_pred, step_idx, dynamic_batch=True):\n",
    "#     '''\n",
    "#     Multi_prediction function.\n",
    "#     :param y_pred: placeholder.\n",
    "#     :param seq: np.ndarray, [len_seq, n_frame, n_route, C_0].\n",
    "#     :param batch_size: int, the size of batch.\n",
    "#     :param n_his: int, size of historical records for training.\n",
    "#     :param n_pred: int, the length of prediction.\n",
    "#     :param step_idx: int or list, index for prediction slice.\n",
    "#     :param dynamic_batch: bool, whether changes the batch size in the last one if its length is less than the default.\n",
    "#     :return y_ : tensor, 'sep' [len_inputs, n_route, 1]; 'merge' [step_idx, len_inputs, n_route, 1].\n",
    "#             len_ : int, the length of prediction.\n",
    "#     '''\n",
    "#     pred_list = []\n",
    "#     for batch in gen_batch(seq, min(batch_size, len(seq)), dynamic_batch=dynamic_batch):\n",
    "#         # Note: use np.copy() to avoid the modification of source data.\n",
    "#         test_seq = torch.copy(batch[:, :, 0:n_his, :])\n",
    "#         step_list = []\n",
    "# #         for j in range(n_pred):\n",
    "#             pred = model(test_seq)\n",
    "# #             pred = sess.run(y_pred,\n",
    "# #                             feed_dict={'data_input:0': , 'keep_prob:0': 1.0})\n",
    "# #             if isinstance(pred, list):\n",
    "# #                 pred = np.array(pred[0])\n",
    "#             # move the window to predict the next 9 time steps\n",
    "# #             test_seq[:, :, 0:n_his - 1, :] = test_seq[:, :, 1:n_his, :]\n",
    "# #             test_seq[:, :, n_his - 1, :] = pred\n",
    "#             step_list.append(pred)\n",
    "#         pred_list.append(step_list)\n",
    "#     #  pred_array -> [n_pred, batch_size, n_route, C_0)\n",
    "#     pred_array = np.concatenate(pred_list, axis=1)\n",
    "#     return pred_array[step_idx], pred_array.shape[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[[1], [2]], [[3], [4]]], [[[5], [6]], [[7], [8]]]]]\n",
      "[[[[1]\n",
      "   [2]]\n",
      "\n",
      "  [[3]\n",
      "   [4]]]\n",
      "\n",
      "\n",
      " [[[5]\n",
      "   [6]]\n",
      "\n",
      "  [[7]\n",
      "   [8]]]]\n"
     ]
    }
   ],
   "source": [
    "a = [[[[[1], [2]], [[3], [4]]], [[[5], [6]], [[7], [8]]]]]\n",
    "print(a)\n",
    "print(np.concatenate(a, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(inputs, batch_size, n_his, n_pred, inf_mode):\n",
    "    '''\n",
    "    Load and test saved model from the checkpoint.\n",
    "    :param inputs: instance of class Dataset, data source for test.\n",
    "    :param batch_size: int, the size of batch.\n",
    "    :param n_his: int, the length of historical records for training.\n",
    "    :param n_pred: int, the length of prediction.\n",
    "    :param inf_mode: str, test mode - 'merge / multi-step test' or 'separate / single-step test'.\n",
    "    :param load_path: str, the path of loaded model.\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = ST_Network(n_hist, ks, kt, rate, Lk, Ko).to(device)\n",
    "    # the load_state_dict() function takes a dictionary object not a path to a saved object\n",
    "    model.load_state_dict(torch.load(\"model.pt\"))\n",
    "        \n",
    "    # to set dropout and batch normalization layers to evaluation mode before running inference\n",
    "    model.eval()\n",
    "    print(f'>> Loading saved model from model.pt ...')\n",
    "\n",
    "    # don't care about the further times at the moment\n",
    "#        if inf_mode == 'sep':\n",
    "#            # for inference mode 'sep', the type of step index is int.\n",
    "#            step_idx = n_pred - 1\n",
    "#            tmp_idx = [step_idx]\n",
    "#        elif inf_mode == 'merge':\n",
    "#            # for inference mode 'merge', the type of step index is np.ndarray.\n",
    "#            step_idx = tmp_idx = np.arange(3, n_pred + 1, 3) - 1\n",
    "#        else:\n",
    "#            raise ValueError(f'ERROR: test mode \"{inf_mode}\" is not defined.')\n",
    "\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    # without making the data into batches we'll have cuda out of memory error\n",
    "#     for batch in gen_batch(seq, min(batch_size, len(seq)), dynamic_batch=dynamic_batch):\n",
    "    x_stats = inputs.get_stats()\n",
    "    data_loader = DataLoader(inputs.get_data('test'),\n",
    "                                 batch_size=10, # where did I define batch size\n",
    "                                 drop_last=True)\n",
    "        # gen_batch(inputs.get_data('train'), batch_size, dynamic_batch=True, shuffle=True)\n",
    "        # enumerate mini batches\n",
    "    for _, x_batch in enumerate(data_loader):\n",
    "        x_batch = x_batch.to(device)\n",
    "#         y_test, len_test = multi_pred(model, pred, x_test, batch_size, n_his, n_pred, step_idx)\n",
    "        pred = model(x_batch)\n",
    "#         evl = evaluation(x_test[0:len_test, step_idx + n_his, :, :], y_test, x_stats)\n",
    "\n",
    "#         for ix in tmp_idx:\n",
    "#             te = evl[ix - 2:ix + 1]\n",
    "#             print(f'Time Step {ix + 1}: MAPE {te[0]:7.3%}; MAE  {te[1]:4.3f}; RMSE {te[2]:6.3f}.')\n",
    "           \n",
    "        loss = loss_fn(x_batch[:, :, n_his, :], pred)\n",
    "        print(loss.item()) #, MAE(v, v_), RMSE(v, v_)])\n",
    "    \n",
    "    print(f'Model Test Time {time.time() - start_time:.3f}s')\n",
    "    print('Testing model finished!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.Lk\n",
      "torch.cuda.FloatTensor\n",
      ">> Loading saved model from model.pt ...\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.18162204325199127\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.14064793288707733\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.12468849867582321\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.1285170614719391\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.1725255846977234\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.20767459273338318\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.6556805968284607\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "1.6254857778549194\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "2.131028890609741\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "1.6703438758850098\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "1.0962355136871338\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.6927815079689026\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.6135085225105286\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.36740604043006897\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.34401923418045044\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.4557764530181885\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "1.0907843112945557\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "1.8844358921051025\n",
      "forward\n",
      "torch.cuda.FloatTensor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raha/anaconda3/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1, 10, 228])) that is different to the input size (torch.Size([10, 1, 228])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "2.402784585952759\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "2.734307050704956\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "2.2338006496429443\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.9177197217941284\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.18007028102874756\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.10295956581830978\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.11660277843475342\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.18243178725242615\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.2041015625\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.1554637998342514\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.12268208712339401\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.12445780634880066\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.12765398621559143\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.19485031068325043\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.21387234330177307\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.9674264192581177\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "2.014026641845703\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "2.046682834625244\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "1.522930383682251\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "1.0685123205184937\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.7151567935943604\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.6063101887702942\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.44812390208244324\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.3775225281715393\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.5043411254882812\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "1.3197779655456543\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "2.3812294006347656\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "3.3686013221740723\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "3.487948179244995\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "2.778961420059204\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "1.2874103784561157\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.3383677005767822\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.10440317541360855\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.13228747248649597\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.16721397638320923\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.167586550116539\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.13217958807945251\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.12602858245372772\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.13141602277755737\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.13950952887535095\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.20998768508434296\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.21397913992404938\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1015839576721191\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "2.1507420539855957\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "2.363149404525757\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "1.5764728784561157\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "1.1639468669891357\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.9660990238189697\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.8622641563415527\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.8671058416366577\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.9036791324615479\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "1.0583223104476929\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "2.314507246017456\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "3.307506561279297\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "3.886306047439575\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "4.21481466293335\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "2.956416130065918\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "1.3438410758972168\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.29867684841156006\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.10289610922336578\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.14007942378520966\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.15757092833518982\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.15337339043617249\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.12350179255008698\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.14778777956962585\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.14425240457057953\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.1632227599620819\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.19968180358409882\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.35984039306640625\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "1.3315385580062866\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "2.393068313598633\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "2.003411054611206\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "1.3137445449829102\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "1.086517572402954\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.9738534688949585\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.9157185554504395\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.8246268033981323\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.8248973488807678\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "1.3712419271469116\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "2.604433536529541\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "3.3713481426239014\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "3.5881502628326416\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "4.033463478088379\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "2.6955559253692627\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "1.1658642292022705\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.3408658802509308\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.13977064192295074\n",
      "forward\n",
      "torch.cuda.FloatTensor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.16329222917556763\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.2326105833053589\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.203561469912529\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.16271266341209412\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.13149623572826385\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.1412031203508377\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.1459171622991562\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.18284323811531067\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.22261393070220947\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.5907819271087646\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.9389266967773438\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.9731074571609497\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.7785304188728333\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.8761425018310547\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.8879126906394958\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "1.0336356163024902\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "1.1173545122146606\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "1.4971376657485962\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "2.7186214923858643\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "3.9391047954559326\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "3.931041717529297\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "3.815791130065918\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "3.076972723007202\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "1.9249267578125\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "1.2220666408538818\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.5909859538078308\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.24688349664211273\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.26785793900489807\n",
      "forward\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "x_tmp\n",
      "torch.cuda.FloatTensor\n",
      "kernel\n",
      "torch.cuda.FloatTensor\n",
      "x_ker\n",
      "torch.cuda.FloatTensor\n",
      "theta\n",
      "torch.cuda.FloatTensor\n",
      "0.37868019938468933\n",
      "Model Test Time 1.275s\n",
      "Testing model finished!\n"
     ]
    }
   ],
   "source": [
    "model_test(PeMS, PeMS.get_len('test'), n_hist, n_pred, inf_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

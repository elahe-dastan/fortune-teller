@inproceedings{1709.04875,
  doi = {10.24963/ijcai.2018/505},
  url = {https://doi.org/10.24963/ijcai.2018/505},
  year = {2018},
  month = jul,
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  author = {Bing Yu and Haoteng Yin and Zhanxing Zhu},
  title = {Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting},
  booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence}
}

@incollection{Vlahogianni2015,
  doi = {10.1007/978-3-319-18320-6_7},
  url = {https://doi.org/10.1007/978-3-319-18320-6_7},
  year = {2015},
  publisher = {Springer International Publishing},
  pages = {107--128},
  author = {Eleni I. Vlahogianni},
  title = {Computational Intelligence and Optimization for Transportation Big Data: Challenges and Opportunities},
  booktitle = {Computational Methods in Applied Sciences}
}

@article{Williams2003,
  doi = {10.1061/(asce)0733-947x(2003)129:6(664)},
  url = {https://doi.org/10.1061/(asce)0733-947x(2003)129:6(664)},
  year = {2003},
  month = nov,
  publisher = {American Society of Civil Engineers ({ASCE})},
  volume = {129},
  number = {6},
  pages = {664--672},
  author = {Billy M. Williams and Lester A. Hoel},
  title = {Modeling and Forecasting Vehicular Traffic Flow as a Seasonal {ARIMA} Process: Theoretical Basis and Empirical Results},
  journal = {Journal of Transportation Engineering}
}

@inproceedings{YuhanJia2016,
  doi = {10.1109/itsc.2016.7795712},
  url = {https://doi.org/10.1109/itsc.2016.7795712},
  year = {2016},
  month = nov,
  publisher = {{IEEE}},
  author = {Yuhan Jia and  Jianping Wu and  Yiman Du},
  title = {Traffic speed prediction using deep learning method},
  booktitle = {2016 {IEEE} 19th International Conference on Intelligent Transportation Systems ({ITSC})}
}

@article{Chen_Song_Yamada_Shibasaki_2016,
  title={Learning Deep Representation from Big and Heterogeneous Data for Traffic Accident Inference},
  volume={30}, url={https://ojs.aaai.org/index.php/AAAI/article/view/10011},
  number={1},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  author={Chen, Quanjun and Song, Xuan and Yamada, Harutoshi and Shibasaki, Ryosuke},
  year={2016},
  month={Feb.}
}

@inproceedings{1506.04214,
  author = {Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-kin and Woo, Wang-chun},
  title = {Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting},
  year = {2015},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
  abstract = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.},
  booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
  pages = {802–810},
  numpages = {9},
  location = {Montreal, Canada},
  series = {NIPS'15}
}

@inproceedings{1409.3215,
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  title = {Sequence to Sequence Learning with Neural Networks},
  year = {2014},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
  pages = {3104–3112},
  numpages = {9},
  location = {Montreal, Canada},
  series = {NIPS'14}
}

@inproceedings{1312.6203,
  title = "Spectral networks and locally connected networks on graphs",
  author = "Joan Bruna and Wojciech Zaremba and Arthur Szlam and Yann Lecun",
  year = "2014",
  language = "English (US)",
  booktitle = "International Conference on Learning Representations (ICLR2014), CBLS, April 2014",
}

@article{2004.08555,
  author={Yin, Xueyan and Wu, Genze and Wei, Jinze and Shen, Yanming and Qi, Heng and Yin, Baocai},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  title={Deep Learning on Traffic Prediction: Methods, Analysis and Future Directions},
  year={2021},
  volume={},
  number={},
  pages={1-17},
  doi={10.1109/TITS.2021.3054840}
}

@article{Rabiner1986,
  doi = {10.1109/massp.1986.1165342},
  url = {https://doi.org/10.1109/massp.1986.1165342},
  year = {1986},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {3},
  number = {1},
  pages = {4--16},
  author = {L. Rabiner and B. Juang},
  title = {An introduction to hidden Markov models},
  journal = {{IEEE} {ASSP} Magazine}
}

@article{1612.01022,
  title={Short-term traffic flow forecasting with spatial-temporal correlation in a hybrid deep learning framework},
  author={Yuankai Wu and Huachun Tan},
  journal={ArXiv},
  year={2016},
  volume={abs/1612.01022}
}

@incollection{Seo2018,
  doi = {10.1007/978-3-030-04167-0_33},
  url = {https://doi.org/10.1007/978-3-030-04167-0_33},
  year = {2018},
  publisher = {Springer International Publishing},
  pages = {362--373},
  author = {Youngjoo Seo and Michaël Defferrard and Pierre Vandergheynst and Xavier Bresson},
  title = {Structured Sequence Modeling with Graph Convolutional Recurrent Networks},
  booktitle = {Neural Information Processing}
}

@article{1707.01926,
  title={Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting},
  author={Yaguang Li and Rose Yu and Cyrus Shahabi and Yan Liu},
  journal={arXiv: Learning},
  year={2018}
}
